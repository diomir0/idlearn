{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bbf1c7-647c-4176-86a5-82424644c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "import shlex\n",
    "import re\n",
    "import markdown2\n",
    "import genanki\n",
    "import pymupdf\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62eeb5-8046-4f0f-bfe5-e3e47d5e786d",
   "metadata": {},
   "source": [
    "# PDF extraction & structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb70ed57-9b32-4dbc-9037-c75cfae9e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = \"/home/diomir0/Documents/papers/Bedford, 2023 - LSD_Connectivity.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060f7f08-3ba1-4018-8f7f-f687fac2f4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format': 'PDF 1.4', 'title': 'The effect of lysergic acid diethylamide (LSD) on whole-brain functional and effective connectivity', 'author': 'Peter Bedford', 'subject': 'Neuropsychopharmacology, doi:10.1038/s41386-023-01574-8', 'keywords': '', 'creator': 'Springer', 'producer': '', 'creationDate': \"D:20230605192020+05'30'\", 'modDate': \"D:20230605192216+05'30'\", 'trapped': '', 'encryption': None}\n"
     ]
    }
   ],
   "source": [
    "# Opening PDF\n",
    "doc = pymupdf.open(pdf)\n",
    "print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751c21ff-3870-433c-9b1b-3233f567191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Table of Contents\n",
    "toc = doc.get_toc()\n",
    "\n",
    "sec_names = [toc[i][1].lower() for i in range(len(toc))]\n",
    "\n",
    "# Extracting the sections \n",
    "sections = []\n",
    "for sec in toc:\n",
    "    if sec[0]==2:\n",
    "        sections.append(sec[1].lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfad6b7-66f9-4fc4-8aad-b13ec3580e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main font size is 9\n"
     ]
    }
   ],
   "source": [
    "# Defining page height from first page\n",
    "pheight = doc[0].rect.height\n",
    "# Defining frame height \n",
    "pframe = 50\n",
    "\n",
    "# Computing dominant text size throughout the document\n",
    "font_sizes = []\n",
    "for page in doc:\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    blocks = text_dict[\"blocks\"]\n",
    "    for block in blocks:\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                font_sizes.append(span[\"size\"])\n",
    "font_count = Counter(font_sizes)\n",
    "dominant_fonts = font_count.most_common(2)\n",
    "\n",
    "if (dominant_fonts[1][0] > dominant_fonts[0][0] and dominant_fonts[1][1] > dominant_fonts[0][0]/2):\n",
    "    main_font = round(dominant_fonts[1][0]) \n",
    "else:\n",
    "    main_font = round(dominant_fonts[0][0]) \n",
    "    \n",
    "print(\"The main font size is {}\".format(main_font))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dfb1383-5cdb-45f2-b71c-7ebe872e82d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting text blocks\n",
    "pblocks = []\n",
    "for page in doc:\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    pblocks.append(text_dict[\"blocks\"])\n",
    "\n",
    "main_text = {}\n",
    "key = \"\"\n",
    "value = \"\"\n",
    "\n",
    "for blocks in pblocks:\n",
    "    for block in blocks:\n",
    "        # Removing header and footer blocks\n",
    "        if block[\"bbox\"][1] > pframe and block[\"bbox\"][3] < pheight-pframe:\n",
    "            for line in block.get(\"lines\", []):\n",
    "                # Get section titles as dict keys\n",
    "                if line.get(\"spans\", [])[0][\"text\"].lower() in sections: \n",
    "                    key = line.get(\"spans\", [])[0][\"text\"].lower()\n",
    "                    value = \"\"\n",
    "                    continue\n",
    "                # If the references title is not included in the toc but still exists\n",
    "                elif (any([sec.lower() == \"references\" for sec in sections]) == False and\n",
    "                         line.get(\"spans\", [])[0][\"text\"].lower() == \"references\"):  \n",
    "                    key = \"references\"\n",
    "                    value = \"\"\n",
    "                    continue\n",
    "                # Excluding figure and table captions\n",
    "                elif (re.match(r'Fig(ure)?\\.(\\s)?(\\d+)?(\\w+)?(\\s+)?:', line.get(\"spans\", [])[0][\"text\"]) or \n",
    "                      re.match(r'Table(\\s)?(\\d+)?(\\w+)?(\\s+)?:', line.get(\"spans\", [])[0][\"text\"])):\n",
    "                    break\n",
    "                # Get the block's text as dict value based on the font size\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    # Introducing a tolerance of font size of 0.5 for small variations in the text body\n",
    "                    if ((round(span[\"size\"]) == main_font and key != \"materials and methods\")  \n",
    "                        and key != \"\"):\n",
    "                        text = span[\"text\"]\n",
    "                        # Repairing lines\n",
    "                        if (len(value) > 1 and (value[-1] == \"-\" or value[-1] == \"ﬁ\" or value[-1] == 'ﬂ') \n",
    "                            or (text == 'ﬁ' or text == 'ﬂ')):\n",
    "                            value = value + text   \n",
    "                        else: \n",
    "                            value = value + ' ' + text \n",
    "                    # Text in an article \"Materials and Methods\" section can have a smaller font size\n",
    "                    if (round(span[\"size\"]) >= main_font-1 and round(span[\"size\"]) <= main_font \n",
    "                         and key == \"materials and methods\"):\n",
    "                        text = span[\"text\"]\n",
    "                        # Repairing lines\n",
    "                        if (len(value) > 1 and (value[-1] == \"-\" or value[-1] == \"ﬁ\" or value[-1] == 'ﬂ') \n",
    "                            or (text == 'ﬁ' or text == 'ﬂ')):\n",
    "                            value = value + text   \n",
    "                        else: \n",
    "                            value = value + ' ' + text \n",
    "                        \n",
    "        if (key != \"\" and value != \"\"):\n",
    "            # Removing references\n",
    "            value = re.sub(r'(;\\s)?\\[\\s(\\d(\\s,\\s+)?)+\\s\\]', '', value.strip())\n",
    "            value = re.sub(r'(;\\s)?\\[\\s\\d+\\s–\\s\\d+\\s\\]', '', value.strip())\n",
    "            # Formatting spaces surrounding commas, dots, and parentheses\n",
    "            value = re.sub(r'\\s,\\s', ', ', value.strip())\n",
    "            value = re.sub(r'\\s\\.\\s', '. ', value.strip())\n",
    "            value = re.sub(r'\\(\\s', '(', value.strip())\n",
    "            value = re.sub(r'\\s\\)', ')', value.strip())\n",
    "            # Removing multiple spaces (strip method fails)\n",
    "            value = re.sub(r'\\s+', ' ', value.strip())\n",
    "            # Replacing the 'ﬁ' and 'ﬂ' characters with correct \"fi\" string\n",
    "            value = re.sub(r'ﬁ', 'fi', value.strip())\n",
    "            value = re.sub(r'ﬂ', 'fl', value.strip())\n",
    "            \n",
    "            main_text[key] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8508420-0683-4f59-81a1-f000845edc16",
   "metadata": {},
   "source": [
    "# LLM Summarizing & Question-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb7a965-5c73-4ade-b7f3-972466288767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling mistral/7B-Instruct from ollama and returning its output\n",
    "def ollama_generate(prompt, model='mistral:7B-instruct'):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    response = requests.post(url, json={\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,  # Set to True if you want streaming responses\n",
    "        \"system\": \"You are a helpful assistant that has insight in academic, theoretical knowledge in science and humanities, and that is able to accurately summarize complex texts concisely yet precisely without skipping important details.\",\n",
    "        \"temperature\": 0.4\n",
    "    })\n",
    "    return response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3e6ab8-3ed9-4812-98e1-f6237d0f2064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download valhalla/t5-base6e2e-qg model from HF - DONE\n",
    "#login()\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"valhalla/t5-base-e2e-qg\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"valhalla/t5-base-e2e-qg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "889e6f5c-efcf-404f-a6ae-64b166f81d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Path to local HF files\n",
    "local_path = \"/home/diomir0/.cache/huggingface/hub/models--valhalla--t5-base-e2e-qg/snapshots/c652651334cd5516f2bd0f0fb5303a01a678024e\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9245570-b165-4f44-ac74-c5a9f237c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling valhalla/t5-base-e2e-qg\n",
    "def qf_generate(text, num, tokenizer = tokenizer, model = model):\n",
    "    input_text = f\"{text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], \n",
    "                             max_new_tokens=128, \n",
    "                             num_return_sequences=num, \n",
    "                             num_beams=num,\n",
    "                             do_sample=False\n",
    "                             )\n",
    "    questions_batch = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "    questions_split = [q.strip().split('<sep>') for q in questions_batch]\n",
    "\n",
    "    # Flatten and clean\n",
    "    final_questions = []\n",
    "    seen = set()\n",
    "    for batch in questions_split:\n",
    "        for q in batch:\n",
    "            q = q.strip()\n",
    "            if q.endswith(\"?\") and len(q.split()) > 3 and q not in seen:\n",
    "                seen.add(q)\n",
    "                final_questions.append(q)\n",
    "\n",
    "    return final_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "314aae28-e04e-450c-bdc2-70f678664a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ollama', 'run', 'mistral:7b-instruct'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting ollama from shell\n",
    "subprocess.run(shlex.split(\"ollama run mistral:7b-instruct\"), check = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870ba244-5d6a-4995-be10-a8b5b95ca99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Initialize the dictionaries\n",
    "sum_dict = {}\n",
    "q_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a10fdd01-8809-4b6b-bcc2-4d3cd396c2f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start of LLM prompt asking for a complete, precise summary yet as concise as possible \n",
    "sum_instruct = \"You are an expert science educator. Given the following text, do two things: 1. Summarize it clearly, precisely and as concisely as precison allows. 2. Then, extract the key concepts or facts as 3–8 concise bullet points. Text: \"\n",
    "questions_instruct = \"You are an expert science and humanities educator. Given the following text, generate a set of five relevant questions and their answers, making sure to only output the questions and their answers in the form of 'Q: ... A:...'. Text: {text}\"\n",
    "\n",
    "for key in main_text.keys():\n",
    "    # Create summary and bullet points of each main_text entry and store it in sum_dict\n",
    "    if not key in sum_dict.keys():\n",
    "        sum_dict[key] = ollama_generate(sum_instruct + main_text[key])\n",
    "    # Generate questions based on the text\n",
    "    if not key in q_dict.keys():\n",
    "        q_dict[key] = ollama_generate(questions_instruct.format(text=main_text[key]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41797fb2-a171-4e52-8cbe-3de56fdf67da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Q: What percentage of unique correlation coefficients in functional connectivity significantly differed between LSD and placebo conditions?\n",
      "A: About 23% (1993/8646) of the unique correlation coefficients significantly differed between LSD and placebo conditions.\n",
      "\n",
      "Q: Where was stronger functional connectivity primarily observed under LSD?\n",
      "A: Stronger functional connectivity was primarily observed in connections involving the bilateral lingual gyrus, bilateral inferior frontal gurys (pars opercularis), right inferior frontal gyrus and right lingual gyrus, left cuneus and right middle frontal gyrus (MFG), and left temporo-occipital middle temporal gyrus (MTG) and left intracalcerine cortex.\n",
      "\n",
      "Q: Where was weaker functional connectivity primarily found under LSD?\n",
      "A: Weaker functional connectivity was primarily found between several occipital regions, including left fusiform gyrus and right inferior lateral occipital cortex (LOC), supracalcarine cortex, occipital pole, and between bilateral puta-men and cerebellum.\n",
      "\n",
      "Q: What did the machine learning analysis of functional connectivity suggest?\n",
      "A: The machine learning analysis suggested that connections involving at least one occipital region or at least one prefrontal region were most relevant for discriminating between LSD and placebo conditions, with the random forest trained on FC as features achieving a BAC (Balance Accuracy) of 86%.\n",
      "\n",
      "Q: What was observed in terms of effective connectivity under LSD compared to placebo?\n",
      "A: Similar to functional connectivity, weaker effective connectivity was primarily found between several occipital regions under LSD, while stronger EC was observed for connections between occipital and prefrontal regions. The effect of LSD on effective connectivity showed a statistically significant condition effect on the first LV, but not on the second LV.\n"
     ]
    }
   ],
   "source": [
    "print(q_dict[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f45c1-4478-4be1-ada3-2cccbdac0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract questions and answers using regex\n",
    "questions = re.findall(r\"Q:\\s.*?(?=\\nA:|\\Z)\", text, re.DOTALL)\n",
    "answers = re.findall(r\"A:\\s.*?(?=\\nQ:|\\Z)\", text, re.DOTALL)\n",
    "\n",
    "# Reformat with Q1:/A1:\n",
    "numbered_questions = [f\"Q{i+1}: {q.strip()[2:].strip()}\" for i, q in enumerate(questions)]\n",
    "numbered_answers = [f\"A{i+1}: {a.strip()[2:].strip()}\" for i, a in enumerate(answers)]\n",
    "\n",
    "# Join into final string\n",
    "final_output = \"\\n\".join(numbered_questions) + \"\\n\\nAnswers:\\n\" + \"\\n\".join(numbered_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc85fc7-0aa4-45eb-949d-aa06802c3be1",
   "metadata": {},
   "source": [
    "# Writing the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d487798f-2c00-4d87-a79f-f0b4390d9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the summary and main concepts of the text (book chapter, article, etc) in a MD file for further use (e.g. upload to Joplin)\n",
    "def write_markdown_file(doc, sum_dict, q_dict, output_path):\n",
    "    numbered_answers = []\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {doc.metadata['title']}\\n\\n\\n\")\n",
    "        for i, key in enumerate(sum_dict.keys(), start=1):\n",
    "            f.write(f\"## {i}. {key.capitalize()}\\n\")\n",
    "            summary = re.sub(r'(1\\.[\\s])?Summary:', '', sum_dict[key])\n",
    "            summary = re.sub(r'(2\\.[\\s])?Key Concepts(/Facts)?(\\:)?\\n(\\n)?', '### Key concepts\\n', summary)\n",
    "            summary = re.sub(r'\\d+\\.\\s', '- ', summary)\n",
    "            f.write(summary.strip() + \"\\n\\n\")\n",
    "            \n",
    "            # Writing questions\n",
    "            f.write(\"### Questions\\n\") \n",
    "            # Extract questions and answers using regex\n",
    "            questions = re.findall(r\"Q:\\s.*?(?=\\nA:|\\Z)\", q_dict[key], re.DOTALL)\n",
    "            answers = re.findall(r\"A:\\s.*?(?=\\n+Q:|\\Z)\", q_dict[key], re.DOTALL)\n",
    "            # Reformat with Q1:/A1:\n",
    "            numbered_questions = [f\"- **Q{i}:** {q.strip()[2:].strip()}\" for i, q in enumerate(questions, start=1)]\n",
    "            numbered_answers.append([f\"- **A{i}:** {a.strip()[2:].strip()}\" for i, a in enumerate(answers, start=1)])\n",
    "            f.write(\"\\n\".join(numbered_questions))\n",
    "            f.write(\"\\n\\n\\n\")\n",
    "\n",
    "        # Writing answers\n",
    "        f.write(\"## Answers\\n\")\n",
    "        for i, key in enumerate(sum_dict.keys(), start=1):\n",
    "            f.write(f\"{i}. {key.capitalize()}\\n\")\n",
    "            f.write(\"\\n\".join(numbered_answers[i-1]))\n",
    "            f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3ae8774-3081-4324-a278-d0e8c230c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_markdown_file(doc, sum_dict, q_dict, f'/home/diomir0/Documents/{doc.metadata['title'].replace(' ', '_')}.md') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf16a7-f053-47a7-b9dc-2bf39f11a921",
   "metadata": {},
   "source": [
    "# Anki card generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d0e7a-0473-4034-8db6-e1752abd9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sum_dict.keys():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
